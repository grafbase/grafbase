// Code generated by wit-bindgen-go. DO NOT EDIT.

// Package kafkaclient represents the imported interface "grafbase:sdk/kafka-client".
package kafkaclient

import (
	"go.bytecodealliance.org/cm"
)

// KafkaSaslPlainAuth represents the record "grafbase:sdk/kafka-client#kafka-sasl-plain-auth".
//
// # SASL PLAIN authentication credentials
//
// Simple username and password authentication. Note that credentials
// are transmitted in base64 encoding, so TLS should be used for security.
//
//	record kafka-sasl-plain-auth {
//		username: string,
//		password: string,
//	}
type KafkaSaslPlainAuth struct {
	_ cm.HostLayout `json:"-"`
	// Username for authentication
	Username string `json:"username"`

	// Password for authentication
	Password string `json:"password"`
}

// KafkaProducerCompression represents the enum "grafbase:sdk/kafka-client#kafka-producer-compression".
//
// # Compression algorithms supported for message payloads
//
// Compression reduces network bandwidth and storage requirements but adds CPU overhead.
// Choose based on your performance requirements and network conditions.
//
//	enum kafka-producer-compression {
//		none,
//		gzip,
//		snappy,
//		lz4,
//		zstd
//	}
type KafkaProducerCompression uint8

const (
	// No compression - fastest but largest message size
	KafkaProducerCompressionNone KafkaProducerCompression = iota

	// GZIP compression - good compression ratio, moderate CPU usage
	KafkaProducerCompressionGzip

	// Snappy compression - fast compression/decompression, moderate compression ratio
	KafkaProducerCompressionSnappy

	// LZ4 compression - very fast, good for high-throughput scenarios
	KafkaProducerCompressionLz4

	// Zstandard compression - excellent compression ratio, configurable speed/ratio trade-off
	KafkaProducerCompressionZstd
)

var _KafkaProducerCompressionStrings = [5]string{
	"none",
	"gzip",
	"snappy",
	"lz4",
	"zstd",
}

// String implements [fmt.Stringer], returning the enum case name of e.
func (e KafkaProducerCompression) String() string {
	return _KafkaProducerCompressionStrings[e]
}

// MarshalText implements [encoding.TextMarshaler].
func (e KafkaProducerCompression) MarshalText() ([]byte, error) {
	return []byte(e.String()), nil
}

// UnmarshalText implements [encoding.TextUnmarshaler], unmarshaling into an enum
// case. Returns an error if the supplied text is not one of the enum cases.
func (e *KafkaProducerCompression) UnmarshalText(text []byte) error {
	return _KafkaProducerCompressionUnmarshalCase(e, text)
}

var _KafkaProducerCompressionUnmarshalCase = cm.CaseUnmarshaler[KafkaProducerCompression](_KafkaProducerCompressionStrings[:])

// KafkaScramMechanism represents the enum "grafbase:sdk/kafka-client#kafka-scram-mechanism".
//
// # SCRAM mechanism variants
//
// Different SHA algorithms used for SCRAM authentication.
// SHA-512 provides stronger security but may have slightly higher CPU overhead.
//
//	enum kafka-scram-mechanism {
//		sha256,
//		sha512
//	}
type KafkaScramMechanism uint8

const (
	// SCRAM-SHA-256 - widely supported, good security
	KafkaScramMechanismSha256 KafkaScramMechanism = iota

	// SCRAM-SHA-512 - stronger security, may have higher CPU overhead
	KafkaScramMechanismSha512
)

var _KafkaScramMechanismStrings = [2]string{
	"sha256",
	"sha512",
}

// String implements [fmt.Stringer], returning the enum case name of e.
func (e KafkaScramMechanism) String() string {
	return _KafkaScramMechanismStrings[e]
}

// MarshalText implements [encoding.TextMarshaler].
func (e KafkaScramMechanism) MarshalText() ([]byte, error) {
	return []byte(e.String()), nil
}

// UnmarshalText implements [encoding.TextUnmarshaler], unmarshaling into an enum
// case. Returns an error if the supplied text is not one of the enum cases.
func (e *KafkaScramMechanism) UnmarshalText(text []byte) error {
	return _KafkaScramMechanismUnmarshalCase(e, text)
}

var _KafkaScramMechanismUnmarshalCase = cm.CaseUnmarshaler[KafkaScramMechanism](_KafkaScramMechanismStrings[:])

// KafkaSaslScramAuth represents the record "grafbase:sdk/kafka-client#kafka-sasl-scram-auth".
//
// # SASL SCRAM authentication credentials
//
// Salted Challenge Response Authentication Mechanism provides stronger
// security than PLAIN by using cryptographic hashing and salts.
//
//	record kafka-sasl-scram-auth {
//		username: string,
//		password: string,
//		mechanism: kafka-scram-mechanism,
//	}
type KafkaSaslScramAuth struct {
	_ cm.HostLayout `json:"-"`
	// Username for authentication
	Username string `json:"username"`

	// Password for authentication
	Password string `json:"password"`

	// SCRAM mechanism variant to use (SHA-256 or SHA-512)
	Mechanism KafkaScramMechanism `json:"mechanism"`
}

// KafkaMtlsAuth represents the record "grafbase:sdk/kafka-client#kafka-mtls-auth".
//
// # Mutual TLS authentication configuration
//
// Uses client certificates for authentication. Both the client certificate
// and private key files must be accessible at the specified paths.
//
//	record kafka-mtls-auth {
//		client-cert-path: string,
//		client-key-path: string,
//	}
type KafkaMtlsAuth struct {
	_ cm.HostLayout `json:"-"`
	// Path to the client certificate file (PEM format)
	ClientCertPath string `json:"client-cert-path"`

	// Path to the client private key file (PEM format)
	ClientKeyPath string `json:"client-key-path"`
}

// KafkaAuthentication represents the variant "grafbase:sdk/kafka-client#kafka-authentication".
//
// # Authentication methods supported by the Kafka client
//
// Kafka supports multiple authentication mechanisms for securing client connections.
// Choose the method that matches your Kafka cluster configuration.
//
//	variant kafka-authentication {
//		sasl-plain(kafka-sasl-plain-auth),
//		sasl-scram(kafka-sasl-scram-auth),
//		mtls(kafka-mtls-auth),
//	}
type KafkaAuthentication cm.Variant[uint8, KafkaSaslScramAuthShape, KafkaSaslScramAuth]

// KafkaAuthenticationSaslPlain returns a [KafkaAuthentication] of case "sasl-plain".
//
// SASL PLAIN authentication - simple username/password authentication
func KafkaAuthenticationSaslPlain(data KafkaSaslPlainAuth) KafkaAuthentication {
	return cm.New[KafkaAuthentication](0, data)
}

// SaslPlain returns a non-nil *[KafkaSaslPlainAuth] if [KafkaAuthentication] represents the variant case "sasl-plain".
func (self *KafkaAuthentication) SaslPlain() *KafkaSaslPlainAuth {
	return cm.Case[KafkaSaslPlainAuth](self, 0)
}

// KafkaAuthenticationSaslScram returns a [KafkaAuthentication] of case "sasl-scram".
//
// SASL SCRAM authentication - challenge-response authentication with password hashing
func KafkaAuthenticationSaslScram(data KafkaSaslScramAuth) KafkaAuthentication {
	return cm.New[KafkaAuthentication](1, data)
}

// SaslScram returns a non-nil *[KafkaSaslScramAuth] if [KafkaAuthentication] represents the variant case "sasl-scram".
func (self *KafkaAuthentication) SaslScram() *KafkaSaslScramAuth {
	return cm.Case[KafkaSaslScramAuth](self, 1)
}

// KafkaAuthenticationMtls returns a [KafkaAuthentication] of case "mtls".
//
// Mutual TLS authentication - certificate-based authentication
func KafkaAuthenticationMtls(data KafkaMtlsAuth) KafkaAuthentication {
	return cm.New[KafkaAuthentication](2, data)
}

// Mtls returns a non-nil *[KafkaMtlsAuth] if [KafkaAuthentication] represents the variant case "mtls".
func (self *KafkaAuthentication) Mtls() *KafkaMtlsAuth {
	return cm.Case[KafkaMtlsAuth](self, 2)
}

var _KafkaAuthenticationStrings = [3]string{
	"sasl-plain",
	"sasl-scram",
	"mtls",
}

// String implements [fmt.Stringer], returning the variant case name of v.
func (v KafkaAuthentication) String() string {
	return _KafkaAuthenticationStrings[v.Tag()]
}

// KafkaConsumerStartOffset represents the variant "grafbase:sdk/kafka-client#kafka-consumer-start-offset".
//
// # Starting offset options for Kafka consumer
//
// Determines where the consumer begins reading messages when no committed offset
// is available. This typically applies to new consumers or when consuming from
// new partitions for the first time.
//
//	variant kafka-consumer-start-offset {
//		earliest,
//		latest,
//		specific(s64),
//	}
type KafkaConsumerStartOffset cm.Variant[uint8, int64, int64]

// KafkaConsumerStartOffsetEarliest returns a [KafkaConsumerStartOffset] of case "earliest".
//
// Start consuming from the earliest available message in the partition
// Useful when you need to process all historical messages
func KafkaConsumerStartOffsetEarliest() KafkaConsumerStartOffset {
	var data struct{}
	return cm.New[KafkaConsumerStartOffset](0, data)
}

// Earliest returns true if [KafkaConsumerStartOffset] represents the variant case "earliest".
func (self *KafkaConsumerStartOffset) Earliest() bool {
	return self.Tag() == 0
}

// KafkaConsumerStartOffsetLatest returns a [KafkaConsumerStartOffset] of case "latest".
//
// Start consuming from the latest message in the partition
// Useful when you only want to process new messages going forward
func KafkaConsumerStartOffsetLatest() KafkaConsumerStartOffset {
	var data struct{}
	return cm.New[KafkaConsumerStartOffset](1, data)
}

// Latest returns true if [KafkaConsumerStartOffset] represents the variant case "latest".
func (self *KafkaConsumerStartOffset) Latest() bool {
	return self.Tag() == 1
}

// KafkaConsumerStartOffsetSpecific returns a [KafkaConsumerStartOffset] of case "specific".
//
// Start consuming from a specific offset position
// Allows precise control over where consumption begins
func KafkaConsumerStartOffsetSpecific(data int64) KafkaConsumerStartOffset {
	return cm.New[KafkaConsumerStartOffset](2, data)
}

// Specific returns a non-nil *[int64] if [KafkaConsumerStartOffset] represents the variant case "specific".
func (self *KafkaConsumerStartOffset) Specific() *int64 {
	return cm.Case[int64](self, 2)
}

var _KafkaConsumerStartOffsetStrings = [3]string{
	"earliest",
	"latest",
	"specific",
}

// String implements [fmt.Stringer], returning the variant case name of v.
func (v KafkaConsumerStartOffset) String() string {
	return _KafkaConsumerStartOffsetStrings[v.Tag()]
}

// KafkaBatchConfig represents the record "grafbase:sdk/kafka-client#kafka-batch-config".
//
// # Kafka producer batching configuration
//
// Controls how messages are batched together before being sent to improve throughput.
// Batching trades off latency for throughput by waiting to accumulate messages
// before sending them to the broker in a single request.
//
//	record kafka-batch-config {
//		linger-ms: u64,
//		batch-size-bytes: u64,
//	}
type KafkaBatchConfig struct {
	_ cm.HostLayout `json:"-"`
	// Maximum time in milliseconds to wait before sending a batch (for batching efficiency)
	LingerMs uint64 `json:"linger-ms"`

	// Maximum size in bytes for a message batch before it's sent
	BatchSizeBytes uint64 `json:"batch-size-bytes"`
}

// KafkaTLSConfig represents the variant "grafbase:sdk/kafka-client#kafka-tls-config".
//
// # TLS configuration options for Kafka connections
//
// Controls whether and how TLS encryption is used when connecting to Kafka brokers.
// Choose the appropriate option based on your security requirements and cluster setup.
//
//	variant kafka-tls-config {
//		system-ca,
//		custom-ca(string),
//	}
type KafkaTLSConfig cm.Variant[uint8, string, string]

// KafkaTLSConfigSystemCa returns a [KafkaTLSConfig] of case "system-ca".
//
// Use TLS with system CA certificates for verification
// This is the recommended option for most production deployments
func KafkaTLSConfigSystemCa() KafkaTLSConfig {
	var data struct{}
	return cm.New[KafkaTLSConfig](0, data)
}

// SystemCa returns true if [KafkaTLSConfig] represents the variant case "system-ca".
func (self *KafkaTLSConfig) SystemCa() bool {
	return self.Tag() == 0
}

// KafkaTLSConfigCustomCa returns a [KafkaTLSConfig] of case "custom-ca".
//
// Use TLS with a custom CA certificate file for verification
// Useful when using self-signed certificates or private CAs
func KafkaTLSConfigCustomCa(data string) KafkaTLSConfig {
	return cm.New[KafkaTLSConfig](1, data)
}

// CustomCa returns a non-nil *[string] if [KafkaTLSConfig] represents the variant case "custom-ca".
func (self *KafkaTLSConfig) CustomCa() *string {
	return cm.Case[string](self, 1)
}

var _KafkaTLSConfigStrings = [2]string{
	"system-ca",
	"custom-ca",
}

// String implements [fmt.Stringer], returning the variant case name of v.
func (v KafkaTLSConfig) String() string {
	return _KafkaTLSConfigStrings[v.Tag()]
}

// KafkaClientConfig represents the record "grafbase:sdk/kafka-client#kafka-client-config".
//
// # General Kafka client configuration options
//
// Contains common configuration settings shared between producers and consumers,
// including partition selection, security settings, and connection parameters.
//
//	record kafka-client-config {
//		partitions: option<list<s32>>,
//		tls: option<kafka-tls-config>,
//		authentication: option<kafka-authentication>,
//	}
type KafkaClientConfig struct {
	_ cm.HostLayout `json:"-"`
	// Specific partitions to consume from (if not specified, consumes from all partitions)
	Partitions cm.Option[cm.List[int32]] `json:"partitions"`

	// TLS configuration for secure communication with Kafka brokers
	TLS cm.Option[KafkaTLSConfig] `json:"tls"`

	// Authentication configuration for connecting to secured Kafka clusters
	Authentication cm.Option[KafkaAuthentication] `json:"authentication"`
}

// KafkaProducerConfig represents the record "grafbase:sdk/kafka-client#kafka-producer-config".
//
// Configuration options for the Kafka producer
//
//	record kafka-producer-config {
//		compression: kafka-producer-compression,
//		batching: option<kafka-batch-config>,
//		client-config: kafka-client-config,
//	}
type KafkaProducerConfig struct {
	_ cm.HostLayout `json:"-"`
	// Compression algorithm to use for message payloads
	Compression KafkaProducerCompression `json:"compression"`

	// Batching configuration to control how messages are grouped before sending
	Batching cm.Option[KafkaBatchConfig] `json:"batching"`

	// General client configuration options (TLS, authentication, partitions)
	ClientConfig KafkaClientConfig `json:"client-config"`
}

// KafkaConsumerConfig represents the record "grafbase:sdk/kafka-client#kafka-consumer-config".
//
// # Configuration options for the Kafka consumer
//
// Controls how the consumer connects to Kafka brokers and consumes messages.
// These settings affect message retrieval behavior, batching, security, and
// starting position when beginning consumption from a topic.
//
//	record kafka-consumer-config {
//		min-batch-size: option<s32>,
//		max-batch-size: option<s32>,
//		max-wait-ms: option<s32>,
//		client-config: kafka-client-config,
//		start-offset: kafka-consumer-start-offset,
//	}
type KafkaConsumerConfig struct {
	_ cm.HostLayout `json:"-"`
	// Minimum number of messages to wait for before returning a batch
	// If not specified, the consumer will return immediately when any messages are available.
	// Setting this helps ensure efficient batching for high-throughput scenarios.
	MinBatchSize cm.Option[int32] `json:"min-batch-size"`

	// Maximum number of messages to return in a single batch
	// Limits memory usage and processing time per batch. If not specified,
	// the consumer may return all available messages up to internal limits.
	MaxBatchSize cm.Option[int32] `json:"max-batch-size"`

	// Maximum time in milliseconds to wait for messages before returning a batch
	// Controls the trade-off between latency and batching efficiency. Lower values
	// reduce latency but may decrease throughput. If not specified, uses reasonable defaults.
	MaxWaitMs cm.Option[int32] `json:"max-wait-ms"`

	// General client configuration options (TLS, authentication, partitions)
	ClientConfig KafkaClientConfig `json:"client-config"`

	// Starting position for message consumption when no previous offset is available
	// Determines where to begin reading messages when starting a new consumer
	StartOffset KafkaConsumerStartOffset `json:"start-offset"`
}

// KafkaProducer represents the imported resource "grafbase:sdk/kafka-client#kafka-producer".
//
// # Kafka producer resource for sending messages to a Kafka topic
//
// The producer maintains a connection to the Kafka cluster and provides
// methods for sending messages with optional keys and configurable delivery semantics.
//
//	resource kafka-producer
type KafkaProducer cm.Resource

// ResourceDrop represents the imported resource-drop for resource "kafka-producer".
//
// Drops a resource handle.
//
//go:nosplit
func (self KafkaProducer) ResourceDrop() {
	self0 := cm.Reinterpret[uint32](self)
	wasmimport_KafkaProducerResourceDrop((uint32)(self0))
	return
}

// KafkaProducerConnect represents the imported static function "connect".
//
// # Create a new Kafka producer and connect to the specified cluster
//
// # Parameters
// - `name`: A unique identifier for the producer instance
// - `servers`: List of Kafka broker addresses (host:port format)
// - `topic`: Name of the Kafka topic to produce messages to
// - `config`: Producer configuration settings
//
// # Returns
// Returns a connected producer instance or an error message if connection fails
//
//	connect: static func(name: string, servers: list<string>, topic: string, config:
//	kafka-producer-config) -> result<kafka-producer, string>
//
//go:nosplit
func KafkaProducerConnect(name string, servers cm.List[string], topic string, config KafkaProducerConfig) (result cm.Result[string, KafkaProducer, string]) {
	params := wasmimport_KafkaProducerConnect_params{name: name, servers: servers, topic: topic, config: config}
	wasmimport_KafkaProducerConnect(&params, &result)
	return
}

// wasmimport_KafkaProducerConnect_params represents the flattened function params for [wasmimport_KafkaProducerConnect].
// See the Canonical ABI flattening rules for more information.
type wasmimport_KafkaProducerConnect_params struct {
	_       cm.HostLayout       `json:"-"`
	name    string              `json:"name"`
	servers cm.List[string]     `json:"servers"`
	topic   string              `json:"topic"`
	config  KafkaProducerConfig `json:"config"`
}

// Produce represents the imported method "produce".
//
// # Send a message to the configured Kafka topic
//
// # Parameters
// - `key`: Optional message key for partitioning and ordering
// - `value`: Message payload as bytes
//
// # Returns
// Returns success or an error message if the message could not be sent
//
//	produce: func(key: option<string>, value: list<u8>) -> result<_, string>
//
//go:nosplit
func (self KafkaProducer) Produce(key cm.Option[string], value cm.List[uint8]) (result cm.Result[string, struct{}, string]) {
	self0 := cm.Reinterpret[uint32](self)
	key0, key1, key2 := lower_OptionString(key)
	value0, value1 := cm.LowerList(value)
	wasmimport_KafkaProducerProduce((uint32)(self0), (uint32)(key0), (*uint8)(key1), (uint32)(key2), (*uint8)(value0), (uint32)(value1), &result)
	return
}

// KafkaMessage represents the record "grafbase:sdk/kafka-client#kafka-message".
//
// # Kafka message representation
//
// Represents a single message consumed from a Kafka topic, containing
// all the metadata and payload associated with the message.
//
//	record kafka-message {
//		offset: s64,
//		key: option<list<u8>>,
//		value: option<list<u8>>,
//		headers: list<tuple<string, list<u8>>>,
//		timestamp: s64,
//		high-watermark: s64,
//	}
type KafkaMessage struct {
	_ cm.HostLayout `json:"-"`
	// The offset of this message within its partition (unique per partition)
	Offset int64 `json:"offset"`

	// Optional message key used for partitioning and message ordering
	Key cm.Option[cm.List[uint8]] `json:"key"`

	// The message payload data
	Value cm.Option[cm.List[uint8]] `json:"value"`

	// Additional metadata headers as key-value pairs
	Headers cm.List[cm.Tuple[string, cm.List[uint8]]] `json:"headers"`

	// Message timestamp in milliseconds since Unix epoch
	Timestamp int64 `json:"timestamp"`

	// Offset that represents the latest message that has been successfully
	// replicated across all in-sync replicas of the partition.
	HighWatermark int64 `json:"high-watermark"`
}

// KafkaConsumer represents the imported resource "grafbase:sdk/kafka-client#kafka-consumer".
//
// # Kafka consumer resource for reading messages from a Kafka topic
//
// The consumer maintains a connection to the Kafka cluster and provides
// methods for retrieving messages from specified partitions with configurable
// batching and offset management.
//
//	resource kafka-consumer
type KafkaConsumer cm.Resource

// ResourceDrop represents the imported resource-drop for resource "kafka-consumer".
//
// Drops a resource handle.
//
//go:nosplit
func (self KafkaConsumer) ResourceDrop() {
	self0 := cm.Reinterpret[uint32](self)
	wasmimport_KafkaConsumerResourceDrop((uint32)(self0))
	return
}

// KafkaConsumerConnect represents the imported static function "connect".
//
// # Create a new Kafka consumer and connect to the specified cluster
//
// # Parameters
// - `servers`: List of Kafka broker addresses (host:port format)
// - `topic`: Name of the Kafka topic to consume messages from
// - `config`: Optional consumer configuration settings
//
// # Returns
// Returns a connected consumer instance or an error message if connection fails
//
//	connect: static func(servers: list<string>, topic: string, config: kafka-consumer-config)
//	-> result<kafka-consumer, string>
//
//go:nosplit
func KafkaConsumerConnect(servers cm.List[string], topic string, config KafkaConsumerConfig) (result cm.Result[string, KafkaConsumer, string]) {
	params := wasmimport_KafkaConsumerConnect_params{servers: servers, topic: topic, config: config}
	wasmimport_KafkaConsumerConnect(&params, &result)
	return
}

// wasmimport_KafkaConsumerConnect_params represents the flattened function params for [wasmimport_KafkaConsumerConnect].
// See the Canonical ABI flattening rules for more information.
type wasmimport_KafkaConsumerConnect_params struct {
	_       cm.HostLayout       `json:"-"`
	servers cm.List[string]     `json:"servers"`
	topic   string              `json:"topic"`
	config  KafkaConsumerConfig `json:"config"`
}

// Next represents the imported method "next".
//
// # Retrieve the next available message from the subscribed topic
//
// This method will block until a message is available or return None
// if no messages are available within the configured timeout period.
//
// # Returns
// Returns the next message if available, None if no messages within timeout,
// or an error message if the operation fails
//
//	next: func() -> result<option<kafka-message>, string>
//
//go:nosplit
func (self KafkaConsumer) Next() (result cm.Result[OptionKafkaMessageShape, cm.Option[KafkaMessage], string]) {
	self0 := cm.Reinterpret[uint32](self)
	wasmimport_KafkaConsumerNext((uint32)(self0), &result)
	return
}
